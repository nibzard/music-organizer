name: Performance Benchmarks

on:
  push:
    branches: [ master, main ]
  pull_request:
    branches: [ master, main ]
  schedule:
    # Run benchmarks daily at 00:00 UTC
    - cron: '0 0 * * *'

jobs:
  benchmark:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12']

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-cov pytest-benchmark

    - name: Run unit tests
      run: |
        python -m pytest tests/ -v --cov=music_organizer --cov-report=xml

    - name: Run performance benchmarks
      run: |
        cd benchmarks
        python run_benchmarks.py

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: benchmark-results-py${{ matrix.python-version }}
        path: benchmarks/benchmark_results.json
        retention-days: 30

    - name: Generate benchmark report
      if: github.event_name == 'pull_request'
      run: |
        # Generate a markdown report for PR comments
        cd benchmarks
        python -c "
import json
import sys
from pathlib import Path

if Path('benchmark_results.json').exists():
    with open('benchmark_results.json') as f:
        results = json.load(f)

    print('## üìä Performance Benchmark Results')
    print('')
    print('| Benchmark | Value | Target | Status |')
    print('|-----------|-------|--------|--------|')

    for r in results:
        status = '‚úÖ PASS' if r['passed'] else '‚ùå FAIL'
        print(f'| {r[\"name\"]} | {r[\"value\"]:.2f} {r[\"unit\"]} | {r[\"target\"]} {r[\"unit\"]} | {status} |')

    passed = sum(1 for r in results if r['passed'])
    total = len(results)

    print('')
    print(f'**Summary**: {passed}/{total} benchmarks passed')
    if passed == total:
        print('üéâ All performance targets achieved!')
    else:
        print('‚ö†Ô∏è Some performance targets not met.')
" > benchmark_report.md

    - name: Comment PR with benchmark results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');

          try {
            const report = fs.readFileSync('benchmarks/benchmark_report.md', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
          } catch (error) {
            console.log('Could not read benchmark report:', error.message);
          }

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  # Compare performance with baseline
  benchmark-comparison:
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'pull_request'

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch full history for comparison

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .

    - name: Download PR benchmark results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results-py3.11
        path: pr-results

    - name: Checkout base branch
      run: |
        git fetch origin ${{ github.base_ref }}
        git checkout origin/${{ github.base_ref }}

    - name: Run benchmarks on base branch
      run: |
        cd benchmarks
        python run_benchmarks.py
        mv benchmark_results.json base_results.json

    - name: Compare results
      run: |
        cd benchmarks
        python -c "
import json
from pathlib import Path

# Load results
with open('../pr-results/benchmark_results.json') as f:
    pr_results = json.load(f)

with open('base_results.json') as f:
    base_results = json.load(f)

# Create lookup dict
base_lookup = {r['name']: r for r in base_results}

print('## üìà Performance Comparison')
print('')
print('| Benchmark | Base | PR | Change |')
print('|-----------|------|----|--------|')

regressions = []
improvements = []

for pr_result in pr_results:
    name = pr_result['name']
    base_result = base_lookup.get(name)

    if base_result:
        base_val = base_result['value']
        pr_val = pr_result['value']

        # Calculate percentage change
        change = ((pr_val - base_val) / base_val) * 100 if base_val != 0 else 0
        change_str = f'{change:+.1f}%'

        # Check for regressions (we want lower values for time/memory, higher for rates)
        if 'Time' in name or 'Memory' in name:
            is_regression = change > 10  # >10% increase is bad
            is_improvement = change < -10  # >10% decrease is good
        else:  # For rates, higher is better
            is_regression = change < -10  # >10% decrease is bad
            is_improvement = change > 10  # >10% increase is good

        if is_regression:
            regressions.append(name)
            change_str = f'**{change_str}** üî¥'
        elif is_improvement:
            improvements.append(name)
            change_str = f'**{change_str}** üü¢'

        print(f'| {name} | {base_val:.2f} | {pr_val:.2f} | {change_str} |')

print('')
if regressions:
    print(f'‚ö†Ô∏è **Performance Regressions**: {len(regressions)}')
    for r in regressions:
        print(f'   - {r}')
    print('')

if improvements:
    print(f'‚ú® **Performance Improvements**: {len(improvements)}')
    for i in improvements:
        print(f'   - {i}')
" > comparison_report.md

    - name: Comment PR with comparison
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');

          try {
            const report = fs.readFileSync('benchmarks/comparison_report.md', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
          } catch (error) {
            console.log('Could not read comparison report:', error.message);
          }

  # Performance regression alerts
  performance-alert:
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'push' && github.ref == 'refs/heads/master'

    steps:
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .

    - name: Download benchmark results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results-py3.11
        path: results

    - name: Check for failures
      run: |
        cd results
        python -c "
import json

with open('benchmark_results.json') as f:
    results = json.load(f)

failed = [r for r in results if not r['passed']]

if failed:
    print('‚ùå Performance benchmarks failed!')
    for f in failed:
        print(f'  - {f[\"name\"]}: {f[\"value\"]:.2f} {f[\"unit\"]} (target: {f[\"target\"]} {f[\"unit\"]})')
    exit(1)
else:
    print('‚úÖ All performance benchmarks passed!')
"